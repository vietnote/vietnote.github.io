[
  {
    "objectID": "notes/gaussian-distribution/gaussian-distritbution.html",
    "href": "notes/gaussian-distribution/gaussian-distritbution.html",
    "title": "Gaussian Distribution",
    "section": "",
    "text": "Introduction\n\\[\n\\begin{equation} \\label{eq:uni-gaussian-pdf}\n\\mathcal{N}(\\mu, \\sigma) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\dfrac{(x-\\mu)^2}{2\\sigma^2})\n\\end{equation}\n\\tag{1}\\]\nAs shown in Equation 1\n\n\nNormalization constant\n\n\nPrinciple of Maximum Entropy\n\n\nMultivariate Gaussian"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#conferences",
    "href": "pubs.html#conferences",
    "title": "Publications",
    "section": "",
    "text": "Performance Insights of Attention-free Language Models in Sentiment Analysis: A Case Study for E-commerce Platforms in Vietnam Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dang N.H. Thanh. 8th International Conference on Inventive Communication and Computational Technologies (ICICCT2024)\nAn Exploratory Comparison of LSTM and BiLSTM in Stock Prediction Nguyen Q. Viet, Nguyen N. Quang, Nguyen King, Dinh T. Huu, Nguyen D. Toan, Dang N.H. Thanh. 7th International Conference on Inventive Communication and Computational Technologies (ICICCT2023)"
  },
  {
    "objectID": "pubs.html#journals",
    "href": "pubs.html#journals",
    "title": "Publications",
    "section": "Journals",
    "text": "Journals\nCustomer Intent Mining from Service Inquiries with Improved Deep Embedded Clustering Nguyen Q.K. Ha, Nguyen T.T. Huyen, Mai T.M. Uyen, Nguyen Q. Viet, Nguyen N. Quang, Dang N.H. Thanh. Journal of Uncertain Systems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viet Nguyen",
    "section": "",
    "text": "About me\nHi, I am currently a junior student majoring in Data Science at University of Economics Ho Chi Minh City (UEH). My research lies in multimodal models  and generative models. I’m also keen to work on models’ interpretability.\n\n\nNews\n\nApr 2024: I received the UEH Young Researcher 2024 Award.\nFeb 2024: Our paper has been accepted at Journal of Uncertain Systems.\nMar 2023: Our paper has been accepted at ICICCT - 2023.\nFeb 2023: I received the UEH Young Researcher 2023 Award.\nAug 2021: I was awarded the UEH Admission Scholarship.\n\n\n\nMedia Coverage\n\nUEH Data Science student published 3 Scopus-indexed papers (in Vietnamese)\n\n\n\nContact\nEmail: quocviethere [at] gmail.com"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nGaussian Distribution\n\n\n\n\n\nThe most widely used distribution for continuous random variables.\n\n\n\n\n\nJul 3, 2024\n\n\n1 min\n\n\n\n\n\n\n\nKL Divergence\n\n\n\n\n\nOne way to measure the distance between two distributions.\n\n\n\n\n\nJul 3, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notes/kl-divergence/kl-divergence.html",
    "href": "notes/kl-divergence/kl-divergence.html",
    "title": "KL Divergence",
    "section": "",
    "text": "Introduction\n\\(f\\)-Divergence:\n\\[\nD_{f}(p_{data} \\| p_\\theta) = \\int p_\\theta(x) f \\left(\\dfrac{p_{data}(x)}{p_\\theta(x)}\\right)dx\n\\]\nwhen \\(f = x\\log x\\), we have the KL Divergence:\n\\[\nD_{KL}(p_{data} \\| p_{\\theta}) = \\int p_{data}(x) \\log \\left(\\dfrac{p_{data}(x)}{p_\\theta(x)}\\right)dx\n\\]\n\n\nChain rule for KL Divergence\n\n\n\nApproximating KL Divergence"
  }
]